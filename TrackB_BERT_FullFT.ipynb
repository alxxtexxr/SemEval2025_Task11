{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "- Multi-Label Classification Model From Scratch: Step-by-Step Tutorial (https://huggingface.co/blog/Valerii-Knowledgator/multi-label-classification)\n",
    "- https://github.com/NielsRogge/Transformers-Tutorials/blob/master/BERT/Fine_tuning_BERT_(and_friends)_for_multi_label_text_classification.ipynb\n",
    "- https://github.com/huggingface/notebooks/blob/main/examples/text_classification.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -U datasets transformers accelerate sentencepiece # evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "# import evaluate\n",
    "from pprint import pprint\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project name: BERT-Base-SE2025T11B-eng-v0.1\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "lang = 'eng'\n",
    "hf_model_id = 'google-bert/bert-base-uncased'\n",
    "# hf_model_id = 'alxxtexxr/BERT-Base-SE2025T11A-eng-v0.3'\n",
    "hf_data_id = 'alxxtexxr/SemEval2025-Task11-Dataset'\n",
    "hf_data_config = 'track_b_eng_70_15_15'\n",
    "project_name = f'BERT-Base-SE2025T11B-{lang}-v0.1'\n",
    "print(\"Project name:\", project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to: 42\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed):\n",
    "    # Set random seed for NumPy\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Set random seed for Torch\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if using multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True  # Ensures deterministic results\n",
    "    torch.backends.cudnn.benchmark = False  # Avoids non-deterministic algorithms\n",
    "\n",
    "    # Set random seed for Transformers\n",
    "    transformers.set_seed(seed)\n",
    "\n",
    "    # Optionally set random seed for sklearn and Python's own random module\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Set random seed for os\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "    print(f\"Random seed set to: {seed}\")\n",
    "\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:99: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26b3cbe7f0c441aca3ca891037744d9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)ssed_data/track_b/eng_70_15_15/train.csv:   0%|          | 0.00/235k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fc722a7d4dd465ba885d66fbebb3600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)cessed_data/track_b/eng_70_15_15/val.csv:   0%|          | 0.00/50.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39bfad8b6bbd4f52826adec7d2a351b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)essed_data/track_b/eng_70_15_15/test.csv:   0%|          | 0.00/48.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "672ad2514a8c49afa98cfa5d0dde6d40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b74a19935cf14326a061bdcc69885adb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating val split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cf5e37fccb04b79bd541f6fe5fdc41a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data columns: ['text', 'anger_0', 'anger_1', 'anger_2', 'anger_3', 'fear_0', 'fear_1', 'fear_2', 'fear_3', 'joy_0', 'joy_1', 'joy_2', 'joy_3', 'sad_0', 'sad_1', 'sad_2', 'sad_3', 'surprise_0', 'surprise_1', 'surprise_2', 'surprise_3']\n",
      "Emotions columns: ['anger_0', 'anger_1', 'anger_2', 'anger_3', 'fear_0', 'fear_1', 'fear_2', 'fear_3', 'joy_0', 'joy_1', 'joy_2', 'joy_3', 'sad_0', 'sad_1', 'sad_2', 'sad_3', 'surprise_0', 'surprise_1', 'surprise_2', 'surprise_3']\n"
     ]
    }
   ],
   "source": [
    "datasets = load_dataset(hf_data_id, hf_data_config)\n",
    "\n",
    "cols = list(datasets['train'].features)\n",
    "emotion_cols = [col for col in cols if col not in ['Unnamed: 0', 'text', 'emotion']]\n",
    "\n",
    "# splits = data_files.keys()\n",
    "# df = {split: pd.DataFrame(datasets[split]) for split in splits}\n",
    "\n",
    "# cols = list(df['train'].columns)\n",
    "print(\"Data columns:\", cols)\n",
    "\n",
    "# emotion_cols = [col for col in cols if col not in ['Unnamed: 0', 'text', 'emotion']]\n",
    "# # neutral_emotion = df['train'][df['train'][emotion_cols].sum(axis=1) == 0]['emotion'].iloc[0]\n",
    "# # emotions = emotion_cols + [neutral_emotion]\n",
    "print(\"Emotions columns:\", emotion_cols)\n",
    "# print()\n",
    "\n",
    "# print(\"Train DF size:\", len(df['train']))\n",
    "# print(\"Validation DF size:\", len(df['val']))\n",
    "# print(\"Testing DF size:\", len(df['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class to ID:\n",
      "\n",
      "{'anger_0': 0,\n",
      " 'anger_1': 1,\n",
      " 'anger_2': 2,\n",
      " 'anger_3': 3,\n",
      " 'fear_0': 4,\n",
      " 'fear_1': 5,\n",
      " 'fear_2': 6,\n",
      " 'fear_3': 7,\n",
      " 'joy_0': 8,\n",
      " 'joy_1': 9,\n",
      " 'joy_2': 10,\n",
      " 'joy_3': 11,\n",
      " 'sad_0': 12,\n",
      " 'sad_1': 13,\n",
      " 'sad_2': 14,\n",
      " 'sad_3': 15,\n",
      " 'surprise_0': 16,\n",
      " 'surprise_1': 17,\n",
      " 'surprise_2': 18,\n",
      " 'surprise_3': 19}\n",
      "\n",
      "ID to Class:\n",
      "\n",
      "{0: 'anger_0',\n",
      " 1: 'anger_1',\n",
      " 2: 'anger_2',\n",
      " 3: 'anger_3',\n",
      " 4: 'fear_0',\n",
      " 5: 'fear_1',\n",
      " 6: 'fear_2',\n",
      " 7: 'fear_3',\n",
      " 8: 'joy_0',\n",
      " 9: 'joy_1',\n",
      " 10: 'joy_2',\n",
      " 11: 'joy_3',\n",
      " 12: 'sad_0',\n",
      " 13: 'sad_1',\n",
      " 14: 'sad_2',\n",
      " 15: 'sad_3',\n",
      " 16: 'surprise_0',\n",
      " 17: 'surprise_1',\n",
      " 18: 'surprise_2',\n",
      " 19: 'surprise_3'}\n"
     ]
    }
   ],
   "source": [
    "class2id = {class_:id for id, class_ in enumerate(emotion_cols)}\n",
    "id2class = {id:class_ for class_, id in class2id.items()}\n",
    "\n",
    "print(\"Class to ID:\\n\")\n",
    "pprint(class2id, width=1)\n",
    "print()\n",
    "print(\"ID to Class:\\n\")\n",
    "pprint(id2class, width=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(hf_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8afcd481d16478eaafb7aa70728f6d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1937 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61e7da7caaa042e2bd3917e4b012c413",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/415 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a47715f20b34b3783501dd753c842f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/416 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['text', 'anger_0', 'anger_1', 'anger_2', 'anger_3', 'fear_0', 'fear_1', 'fear_2', 'fear_3', 'joy_0', 'joy_1', 'joy_2', 'joy_3', 'sad_0', 'sad_1', 'sad_2', 'sad_3', 'surprise_0', 'surprise_1', 'surprise_2', 'surprise_3', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 1937\n",
       " }),\n",
       " 'val': Dataset({\n",
       "     features: ['text', 'anger_0', 'anger_1', 'anger_2', 'anger_3', 'fear_0', 'fear_1', 'fear_2', 'fear_3', 'joy_0', 'joy_1', 'joy_2', 'joy_3', 'sad_0', 'sad_1', 'sad_2', 'sad_3', 'surprise_0', 'surprise_1', 'surprise_2', 'surprise_3', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 415\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['text', 'anger_0', 'anger_1', 'anger_2', 'anger_3', 'fear_0', 'fear_1', 'fear_2', 'fear_3', 'joy_0', 'joy_1', 'joy_2', 'joy_3', 'sad_0', 'sad_1', 'sad_2', 'sad_3', 'surprise_0', 'surprise_1', 'surprise_2', 'surprise_3', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 416\n",
       " })}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_labels(data, emotion_cols):\n",
    "   return [float(data[emotion_col]) for emotion_col in emotion_cols]\n",
    "\n",
    "def preprocess_function(data):\n",
    "   text = data['text']\n",
    "   labels = get_labels(data, emotion_cols)\n",
    "   data = tokenizer(text, truncation=True)\n",
    "   data['labels'] = labels\n",
    "   return data\n",
    "\n",
    "tokenized_datasets = {split: datasets[split].map(preprocess_function) for split in ['train', 'val', 'test']}\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Just got x-rays, and at least my knee does not appear to be broken ( although they are sending the x-rays to a specialist to be sure ).\n",
      "Labels: [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0] --> ['anger_0', 'anger_1', 'anger_2', 'anger_3', 'fear_0', 'fear_1', 'fear_2', 'fear_3', 'joy_0', 'joy_1', 'joy_2', 'joy_3', 'sad_0', 'sad_1', 'sad_2', 'sad_3', 'surprise_0', 'surprise_1', 'surprise_2', 'surprise_3']\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "data0 = tokenized_datasets['train'][0]\n",
    "\n",
    "print(\"Text:\", data0['text'])\n",
    "print(\"Labels:\", data0['labels'], '-->', emotion_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    hf_model_id, \n",
    "    num_labels=len(emotion_cols),\n",
    "    id2label=id2class, label2id=class2id,\n",
    "    problem_type='multi_label_classification',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])\n",
    "\n",
    "def sigmoid(x):\n",
    "   return 1/(1 + np.exp(-x))\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    probs = sigmoid(predictions)\n",
    "    y_pred = (probs > 0.5).astype(int)\n",
    "    y_true = labels.astype(int)\n",
    "\n",
    "    # Compute F1 score for each type of averaging method\n",
    "    f1_micro = f1_score(y_true, y_pred, average='micro', zero_division=0.0)\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro', zero_division=0.0)\n",
    "    # f1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0.0)\n",
    "    # f1_samples = f1_score(y_true, y_pred, average='samples', zero_division=0.0)\n",
    "    f1_labels = f1_score(y_true, y_pred, average=None, zero_division=0.0)\n",
    "    f1_labels_dict = {f'f1_label_{emotion_cols[i]}': f1_labels[i] for i in range(len(f1_labels))}\n",
    "\n",
    "    return {\n",
    "        'f1_micro': f1_micro,\n",
    "        'f1_macro': f1_macro,\n",
    "        **f1_labels_dict,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-d7f5c93b8dd2>:23: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "train_args = TrainingArguments(\n",
    "    # Training config\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "\n",
    "    # Logging config for training\n",
    "    logging_strategy='steps',\n",
    "    logging_steps=50,\n",
    "\n",
    "    # Evaluation config during training\n",
    "    per_device_eval_batch_size=2,\n",
    "    eval_strategy='steps',\n",
    "    eval_steps=50,\n",
    "\n",
    "    # Model saving config\n",
    "    output_dir=project_name,\n",
    "    save_strategy='epoch',\n",
    "    # load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['val'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malimtegar\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20241027_142818-l87i4s4b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alimtegar/huggingface/runs/l87i4s4b' target=\"_blank\">BERT-Base-SE2025T11B-eng-v0.1</a></strong> to <a href='https://wandb.ai/alimtegar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alimtegar/huggingface' target=\"_blank\">https://wandb.ai/alimtegar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alimtegar/huggingface/runs/l87i4s4b' target=\"_blank\">https://wandb.ai/alimtegar/huggingface/runs/l87i4s4b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1938' max='1938' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1938/1938 04:16, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Label Anger 0</th>\n",
       "      <th>F1 Label Anger 1</th>\n",
       "      <th>F1 Label Anger 2</th>\n",
       "      <th>F1 Label Anger 3</th>\n",
       "      <th>F1 Label Fear 0</th>\n",
       "      <th>F1 Label Fear 1</th>\n",
       "      <th>F1 Label Fear 2</th>\n",
       "      <th>F1 Label Fear 3</th>\n",
       "      <th>F1 Label Joy 0</th>\n",
       "      <th>F1 Label Joy 1</th>\n",
       "      <th>F1 Label Joy 2</th>\n",
       "      <th>F1 Label Joy 3</th>\n",
       "      <th>F1 Label Sad 0</th>\n",
       "      <th>F1 Label Sad 1</th>\n",
       "      <th>F1 Label Sad 2</th>\n",
       "      <th>F1 Label Sad 3</th>\n",
       "      <th>F1 Label Surprise 0</th>\n",
       "      <th>F1 Label Surprise 1</th>\n",
       "      <th>F1 Label Surprise 2</th>\n",
       "      <th>F1 Label Surprise 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.586200</td>\n",
       "      <td>0.479669</td>\n",
       "      <td>0.686038</td>\n",
       "      <td>0.201388</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.576000</td>\n",
       "      <td>0.029197</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.859890</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.805755</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.821023</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.435200</td>\n",
       "      <td>0.390799</td>\n",
       "      <td>0.669344</td>\n",
       "      <td>0.171385</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.859890</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.810888</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.821023</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.380700</td>\n",
       "      <td>0.374277</td>\n",
       "      <td>0.691958</td>\n",
       "      <td>0.201410</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.600509</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.859890</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.810888</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.821023</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.376000</td>\n",
       "      <td>0.367024</td>\n",
       "      <td>0.669344</td>\n",
       "      <td>0.171385</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.859890</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.810888</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.821023</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.364125</td>\n",
       "      <td>0.669191</td>\n",
       "      <td>0.171750</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.859890</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.821023</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.373800</td>\n",
       "      <td>0.355381</td>\n",
       "      <td>0.689564</td>\n",
       "      <td>0.196176</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.478088</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.859890</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.828614</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.821023</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.358700</td>\n",
       "      <td>0.352302</td>\n",
       "      <td>0.705344</td>\n",
       "      <td>0.207935</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707022</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.859890</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.834862</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.821023</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.373200</td>\n",
       "      <td>0.341890</td>\n",
       "      <td>0.697161</td>\n",
       "      <td>0.202940</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.613240</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.859890</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.828746</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.821023</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.341400</td>\n",
       "      <td>0.340273</td>\n",
       "      <td>0.696842</td>\n",
       "      <td>0.201779</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.585185</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.859890</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.833583</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.821023</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.333200</td>\n",
       "      <td>0.335232</td>\n",
       "      <td>0.700080</td>\n",
       "      <td>0.205347</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.655172</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.859890</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.834951</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.821023</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.323400</td>\n",
       "      <td>0.329934</td>\n",
       "      <td>0.701980</td>\n",
       "      <td>0.206115</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.876404</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.840336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.821023</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.328900</td>\n",
       "      <td>0.322079</td>\n",
       "      <td>0.707032</td>\n",
       "      <td>0.210182</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.706949</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.889580</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.834753</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.836469</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.334600</td>\n",
       "      <td>0.321308</td>\n",
       "      <td>0.712442</td>\n",
       "      <td>0.214039</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.069767</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.896047</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.859451</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.333800</td>\n",
       "      <td>0.327853</td>\n",
       "      <td>0.687397</td>\n",
       "      <td>0.195616</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.450216</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.868239</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.821239</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.836735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.321200</td>\n",
       "      <td>0.313875</td>\n",
       "      <td>0.709890</td>\n",
       "      <td>0.211877</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.681672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023529</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.899408</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.832753</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.864275</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.324700</td>\n",
       "      <td>0.312058</td>\n",
       "      <td>0.714247</td>\n",
       "      <td>0.217704</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.735043</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.879499</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.841424</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.866980</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.324300</td>\n",
       "      <td>0.309422</td>\n",
       "      <td>0.711660</td>\n",
       "      <td>0.210915</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.688742</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.891176</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.838028</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.864458</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.316200</td>\n",
       "      <td>0.307398</td>\n",
       "      <td>0.715066</td>\n",
       "      <td>0.211299</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.678082</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.898975</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.848080</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.864947</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.313200</td>\n",
       "      <td>0.304785</td>\n",
       "      <td>0.719132</td>\n",
       "      <td>0.218361</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.715655</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025316</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.912121</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.831008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.875796</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.298300</td>\n",
       "      <td>0.301501</td>\n",
       "      <td>0.722162</td>\n",
       "      <td>0.217677</td>\n",
       "      <td>0.936937</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.735632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.919753</td>\n",
       "      <td>0.048193</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.870918</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.269800</td>\n",
       "      <td>0.297204</td>\n",
       "      <td>0.720524</td>\n",
       "      <td>0.218411</td>\n",
       "      <td>0.935733</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.738854</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.912651</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.842454</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.293600</td>\n",
       "      <td>0.300610</td>\n",
       "      <td>0.713418</td>\n",
       "      <td>0.212564</td>\n",
       "      <td>0.938303</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.642336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.904832</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.839344</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.878837</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.277300</td>\n",
       "      <td>0.304006</td>\n",
       "      <td>0.708727</td>\n",
       "      <td>0.209841</td>\n",
       "      <td>0.938303</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.609023</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.891117</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.837696</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.877193</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.277000</td>\n",
       "      <td>0.292627</td>\n",
       "      <td>0.722298</td>\n",
       "      <td>0.226940</td>\n",
       "      <td>0.939511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.754491</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.916793</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.872964</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.253800</td>\n",
       "      <td>0.291207</td>\n",
       "      <td>0.722799</td>\n",
       "      <td>0.231578</td>\n",
       "      <td>0.939511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.763975</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065934</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.913505</td>\n",
       "      <td>0.025316</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.849498</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.865132</td>\n",
       "      <td>0.208696</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.267900</td>\n",
       "      <td>0.292520</td>\n",
       "      <td>0.722735</td>\n",
       "      <td>0.220791</td>\n",
       "      <td>0.938303</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.741840</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.915408</td>\n",
       "      <td>0.048193</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.851613</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.875606</td>\n",
       "      <td>0.021053</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.292800</td>\n",
       "      <td>0.294204</td>\n",
       "      <td>0.712751</td>\n",
       "      <td>0.227649</td>\n",
       "      <td>0.938303</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.700337</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.831541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.868463</td>\n",
       "      <td>0.114286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.278200</td>\n",
       "      <td>0.292018</td>\n",
       "      <td>0.716097</td>\n",
       "      <td>0.232632</td>\n",
       "      <td>0.940568</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.706667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156863</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.903418</td>\n",
       "      <td>0.025316</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.838938</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.873977</td>\n",
       "      <td>0.206897</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.279000</td>\n",
       "      <td>0.288928</td>\n",
       "      <td>0.724708</td>\n",
       "      <td>0.234356</td>\n",
       "      <td>0.938144</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.763314</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.912173</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.852843</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.875399</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.280800</td>\n",
       "      <td>0.295538</td>\n",
       "      <td>0.712739</td>\n",
       "      <td>0.222040</td>\n",
       "      <td>0.939948</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.644928</td>\n",
       "      <td>0.014815</td>\n",
       "      <td>0.023529</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.897547</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.855172</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.875208</td>\n",
       "      <td>0.189655</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.289400</td>\n",
       "      <td>0.286639</td>\n",
       "      <td>0.722905</td>\n",
       "      <td>0.234206</td>\n",
       "      <td>0.941634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.746082</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023529</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.911011</td>\n",
       "      <td>0.069767</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.849587</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.874791</td>\n",
       "      <td>0.267717</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.278700</td>\n",
       "      <td>0.285979</td>\n",
       "      <td>0.723229</td>\n",
       "      <td>0.235826</td>\n",
       "      <td>0.940260</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.736508</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.912121</td>\n",
       "      <td>0.147368</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.858108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.876254</td>\n",
       "      <td>0.245902</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.274100</td>\n",
       "      <td>0.288034</td>\n",
       "      <td>0.719182</td>\n",
       "      <td>0.229803</td>\n",
       "      <td>0.940260</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.704698</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044944</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.909910</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.849220</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.880914</td>\n",
       "      <td>0.194690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.268400</td>\n",
       "      <td>0.288245</td>\n",
       "      <td>0.720241</td>\n",
       "      <td>0.223808</td>\n",
       "      <td>0.941935</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.724919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022989</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.908271</td>\n",
       "      <td>0.072289</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.854671</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.871875</td>\n",
       "      <td>0.079208</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.289700</td>\n",
       "      <td>0.286275</td>\n",
       "      <td>0.718879</td>\n",
       "      <td>0.231277</td>\n",
       "      <td>0.941330</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.723127</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063830</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.911011</td>\n",
       "      <td>0.069767</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.851211</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.875606</td>\n",
       "      <td>0.189655</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.285800</td>\n",
       "      <td>0.283929</td>\n",
       "      <td>0.723497</td>\n",
       "      <td>0.237917</td>\n",
       "      <td>0.941482</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.753086</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063830</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.911043</td>\n",
       "      <td>0.112360</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.852349</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.882255</td>\n",
       "      <td>0.241935</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.252300</td>\n",
       "      <td>0.284007</td>\n",
       "      <td>0.725239</td>\n",
       "      <td>0.237671</td>\n",
       "      <td>0.941634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.763636</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063830</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.912977</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.850847</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.881579</td>\n",
       "      <td>0.245902</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.270900</td>\n",
       "      <td>0.283991</td>\n",
       "      <td>0.724646</td>\n",
       "      <td>0.239742</td>\n",
       "      <td>0.939198</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.749226</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.122449</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.912977</td>\n",
       "      <td>0.091954</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.856187</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.878939</td>\n",
       "      <td>0.243902</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1938, training_loss=0.3184622050069803, metrics={'train_runtime': 270.2634, 'train_samples_per_second': 14.334, 'train_steps_per_second': 7.171, 'total_flos': 55533114532104.0, 'train_loss': 0.3184622050069803, 'epoch': 2.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.2822621762752533,\n",
       " 'eval_f1_micro': 0.7237311385459534,\n",
       " 'eval_f1_macro': 0.23613747684154168,\n",
       " 'eval_f1_label_anger_0': 0.9391979301423027,\n",
       " 'eval_f1_label_anger_1': 0.0,\n",
       " 'eval_f1_label_anger_2': 0.0,\n",
       " 'eval_f1_label_anger_3': 0.0,\n",
       " 'eval_f1_label_fear_0': 0.7125,\n",
       " 'eval_f1_label_fear_1': 0.0,\n",
       " 'eval_f1_label_fear_2': 0.1276595744680851,\n",
       " 'eval_f1_label_fear_3': 0.0,\n",
       " 'eval_f1_label_joy_0': 0.9138461538461539,\n",
       " 'eval_f1_label_joy_1': 0.0,\n",
       " 'eval_f1_label_joy_2': 0.0,\n",
       " 'eval_f1_label_joy_3': 0.0,\n",
       " 'eval_f1_label_sad_0': 0.8735244519392917,\n",
       " 'eval_f1_label_sad_1': 0.0,\n",
       " 'eval_f1_label_sad_2': 0.0,\n",
       " 'eval_f1_label_sad_3': 0.0,\n",
       " 'eval_f1_label_surprise_0': 0.8795986622073578,\n",
       " 'eval_f1_label_surprise_1': 0.2764227642276423,\n",
       " 'eval_f1_label_surprise_2': 0.0,\n",
       " 'eval_f1_label_surprise_3': 0.0,\n",
       " 'eval_runtime': 2.4533,\n",
       " 'eval_samples_per_second': 169.568,\n",
       " 'eval_steps_per_second': 84.784,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset=tokenized_datasets['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anger', 'fear', 'joy', 'sad', 'surprise']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions = sorted(set(item.split('_')[0] for item in emotion_cols))\n",
    "emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: and Christian fish bumper stickers, it caught my eye and made me grin.\n",
      "\n",
      "True emotion(s):\n",
      "- anger: 0\n",
      "- fear: 0\n",
      "- joy: 2\n",
      "- sad: 0\n",
      "- surprise: 1\n",
      "\n",
      "Predicted emotion(s):\n",
      "- anger: 0\n",
      "- fear: 0\n",
      "- joy: 1\n",
      "- sad: 0\n",
      "- surprise: 0\n"
     ]
    }
   ],
   "source": [
    "def labels2intensities(labels):\n",
    "    return torch.argmax(torch.tensor(labels).reshape(5, 4), dim=1)\n",
    "\n",
    "def print_emotion_intensiies(emotions, intensities):    \n",
    "    for emotion, intensity in zip(emotions, intensities):\n",
    "        print(f\"- {emotion}:\", intensity.item())\n",
    "\n",
    "data = datasets['val'][0]\n",
    "text = data['text']\n",
    "labels_true = get_labels(data, emotion_cols)\n",
    "intensities_true = labels2intensities(labels_true)\n",
    "\n",
    "inputs = tokenizer(text, return_tensors='pt').to(model.device)\n",
    "\n",
    "outputs = trainer.model(**inputs)\n",
    "logits = outputs.logits\n",
    "probs = sigmoid(logits.squeeze().detach().cpu().numpy()) # apply sigmoid + threshold\n",
    "labels_pred = (probs > 0.5).astype(int)\n",
    "intensities_pred = labels2intensities(labels_pred)\n",
    "\n",
    "print(\"Text:\", text)\n",
    "print()\n",
    "print(\"True emotion(s):\")\n",
    "print_emotion_intensiies(emotions, intensities_true)\n",
    "print()\n",
    "print(\"Predicted emotion(s):\")\n",
    "print_emotion_intensiies(emotions, intensities_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
