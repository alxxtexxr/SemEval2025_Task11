{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "from sklearn.model_selection import train_test_split\n",
    "from huggingface_hub import HfApi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face dataset config: sun_go_emotions_70_15_15_back_translated\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "# lang = 'sun'\n",
    "lang = 'sun_go_emotions'\n",
    "\n",
    "raw_data_path = 'data/augmented_data/train/track_a/sun_go_emotions_back_translated.csv'\n",
    "preprocessed_data_dir = './data/preprocessed_data/'\n",
    "\n",
    "split_sizes = [0.7, 0.15, 0.15]\n",
    "assert len(split_sizes) == 3 and sum(split_sizes) == 1.0\n",
    "\n",
    "hf_data_id = 'alxxtexxr/SemEval2025-Task11-Dataset'\n",
    "hf_data_config = lang + '_' + '_'.join([str(int(split_size * 100)) for split_size in split_sizes]) + '_back_translated'\n",
    "print(\"Hugging Face dataset config:\", hf_data_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to: 42\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed):\n",
    "    # Set random seed for NumPy\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Optionally set random seed for sklearn and Python's own random module\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Set random seed for os\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "    print(f\"Random seed set to: {seed}\")\n",
    "\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw DF length: 2494\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>marah</th>\n",
       "      <th>jijik</th>\n",
       "      <th>takut</th>\n",
       "      <th>senang</th>\n",
       "      <th>sedih</th>\n",
       "      <th>terkejut</th>\n",
       "      <th>aug_go_emotions</th>\n",
       "      <th>biasa</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sun_train_track_a_00001</td>\n",
       "      <td>Kumaha engke carita fiksi ka komunitas, salam ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sun_train_track_a_00002</td>\n",
       "      <td>Tapi anak domba eta sigana lain aing nu boga. ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sun_train_track_a_00003</td>\n",
       "      <td>Aduh kang kunaon kudu penting diunggah mah</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sun_train_track_a_00004</td>\n",
       "      <td>Pokokna hapunten ageung, urang terus ningkatke...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sun_train_track_a_00005</td>\n",
       "      <td>Naha manehna teu era?? Na memang tukang nyieun...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2489</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Nya, anjeun geus meakkeun dua jam.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2490</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Kuring teu maca artikel anjeun tapi kuring nya...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2491</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Aya sababaraha variasi sapanjang taun.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2492</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Kuring mendakan sahanteuna hiji putri duyung u...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2493</th>\n",
       "      <td>NaN</td>\n",
       "      <td>teu aya anu husus. eta beda-beda.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2494 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           id  \\\n",
       "0     sun_train_track_a_00001   \n",
       "1     sun_train_track_a_00002   \n",
       "2     sun_train_track_a_00003   \n",
       "3     sun_train_track_a_00004   \n",
       "4     sun_train_track_a_00005   \n",
       "...                       ...   \n",
       "2489                      NaN   \n",
       "2490                      NaN   \n",
       "2491                      NaN   \n",
       "2492                      NaN   \n",
       "2493                      NaN   \n",
       "\n",
       "                                                   text  marah  jijik  takut  \\\n",
       "0     Kumaha engke carita fiksi ka komunitas, salam ...      0      0      0   \n",
       "1     Tapi anak domba eta sigana lain aing nu boga. ...      0      0      0   \n",
       "2            Aduh kang kunaon kudu penting diunggah mah      0      0      0   \n",
       "3     Pokokna hapunten ageung, urang terus ningkatke...      0      0      0   \n",
       "4     Naha manehna teu era?? Na memang tukang nyieun...      0      0      0   \n",
       "...                                                 ...    ...    ...    ...   \n",
       "2489                 Nya, anjeun geus meakkeun dua jam.      0      0      0   \n",
       "2490  Kuring teu maca artikel anjeun tapi kuring nya...      0      0      0   \n",
       "2491             Aya sababaraha variasi sapanjang taun.      0      0      0   \n",
       "2492  Kuring mendakan sahanteuna hiji putri duyung u...      0      0      0   \n",
       "2493                  teu aya anu husus. eta beda-beda.      0      0      0   \n",
       "\n",
       "      senang  sedih  terkejut  aug_go_emotions  biasa emotion  \n",
       "0          1      0         1                0    NaN     NaN  \n",
       "1          1      0         1                0    NaN     NaN  \n",
       "2          1      0         0                0    NaN     NaN  \n",
       "3          1      1         0                0    NaN     NaN  \n",
       "4          1      0         1                0    NaN     NaN  \n",
       "...      ...    ...       ...              ...    ...     ...  \n",
       "2489       0      0         0                1    1.0     NaN  \n",
       "2490       0      0         0                1    1.0     NaN  \n",
       "2491       0      0         0                1    1.0     NaN  \n",
       "2492       0      0         0                1    1.0     NaN  \n",
       "2493       0      0         0                1    1.0     NaN  \n",
       "\n",
       "[2494 rows x 11 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(raw_data_path)\n",
    "print(\"Raw DF length:\", len(df))\n",
    "print()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion columns: ['marah', 'jijik', 'takut', 'senang', 'sedih', 'terkejut', 'biasa']\n",
      "Augmentation columns: ['aug_go_emotions']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>marah</th>\n",
       "      <th>jijik</th>\n",
       "      <th>takut</th>\n",
       "      <th>senang</th>\n",
       "      <th>sedih</th>\n",
       "      <th>terkejut</th>\n",
       "      <th>aug_go_emotions</th>\n",
       "      <th>biasa</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sun_train_track_a_00001</td>\n",
       "      <td>Kumaha engke carita fiksi ka komunitas, salam ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>senang, terkejut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sun_train_track_a_00002</td>\n",
       "      <td>Tapi anak domba eta sigana lain aing nu boga. ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>senang, terkejut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sun_train_track_a_00003</td>\n",
       "      <td>Aduh kang kunaon kudu penting diunggah mah</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>senang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sun_train_track_a_00004</td>\n",
       "      <td>Pokokna hapunten ageung, urang terus ningkatke...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>senang, sedih</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sun_train_track_a_00005</td>\n",
       "      <td>Naha manehna teu era?? Na memang tukang nyieun...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>senang, terkejut</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id                                               text  \\\n",
       "0  sun_train_track_a_00001  Kumaha engke carita fiksi ka komunitas, salam ...   \n",
       "1  sun_train_track_a_00002  Tapi anak domba eta sigana lain aing nu boga. ...   \n",
       "2  sun_train_track_a_00003         Aduh kang kunaon kudu penting diunggah mah   \n",
       "3  sun_train_track_a_00004  Pokokna hapunten ageung, urang terus ningkatke...   \n",
       "4  sun_train_track_a_00005  Naha manehna teu era?? Na memang tukang nyieun...   \n",
       "\n",
       "   marah  jijik  takut  senang  sedih  terkejut  aug_go_emotions  biasa  \\\n",
       "0      0      0      0       1      0         1                0      0   \n",
       "1      0      0      0       1      0         1                0      0   \n",
       "2      0      0      0       1      0         0                0      0   \n",
       "3      0      0      0       1      1         0                0      0   \n",
       "4      0      0      0       1      0         1                0      0   \n",
       "\n",
       "            emotion  \n",
       "0  senang, terkejut  \n",
       "1  senang, terkejut  \n",
       "2            senang  \n",
       "3     senang, sedih  \n",
       "4  senang, terkejut  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_col_map = {\n",
    "    'eng': { 'Anger': 'anger', 'Fear': 'fear', 'Joy': 'joy', 'Sadness': 'sad', 'Surprise': 'surprise' },\n",
    "    'deu': { 'Anger': 'wut', 'Disgust': 'ekel', 'Fear': 'angst', 'Joy': 'freude', 'Sadness': 'trauer', 'Surprise': 'Ã¼berraschung' },\n",
    "    'sun': { 'Anger': 'marah', 'Disgust': 'jijik', 'Fear': 'takut', 'Joy': 'senang', 'Sadness': 'sedih', 'Surprise': 'terkejut' },\n",
    "    'sun_go_emotions': { 'Anger': 'marah', 'Disgust': 'jijik', 'Fear': 'takut', 'Joy': 'senang', 'Sadness': 'sedih', 'Surprise': 'terkejut' },\n",
    "}\n",
    "emotion_cols = list(emotion_col_map[lang].values())\n",
    "\n",
    "neutral_emotion_map = {\n",
    "    'eng': 'neutral',\n",
    "    'deu': 'neutral',\n",
    "    'sun': 'biasa',\n",
    "    'sun_go_emotions': 'biasa',\n",
    "}\n",
    "neutral_emotion = neutral_emotion_map[lang]\n",
    "\n",
    "# Rename emotion columns\n",
    "df = df.rename(columns=emotion_col_map[lang])\n",
    "\n",
    "# Create 'emotion' column by combining the positive emotions\n",
    "df['emotion'] = df.apply(lambda row: ', '.join([col for col in emotion_cols if row[col] == 1]), axis=1)\n",
    "df['emotion'] = df['emotion'].replace('', neutral_emotion) # Fill neutral emotion\n",
    "\n",
    "# Create neutral emotion column\n",
    "df[neutral_emotion_map[lang]] = (df['emotion'] == neutral_emotion).astype(int)\n",
    "emotion_cols += [neutral_emotion]\n",
    "print(\"Emotion columns:\", emotion_cols)\n",
    "\n",
    "# Get augmentation columns\n",
    "aug_columns = [col for col in df.columns if col.startswith('aug_')]\n",
    "print(\"Augmentation columns:\", aug_columns)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data with non-ASCII chars: 0\n"
     ]
    }
   ],
   "source": [
    "def contains_non_ascii(text):\n",
    "    try:\n",
    "        text.encode('ascii')\n",
    "    except UnicodeEncodeError:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# def remove_emojis_and_symbols(text):\n",
    "#     # Regex pattern to match emojis and symbols with variation selectors\n",
    "#     emoji_pattern = re.compile(\n",
    "#         \"[\\U00010000-\\U0010ffff\\U00002000-\\U00002BFF\\U00002702-\\U000027B0]+\", \n",
    "#         flags=re.UNICODE)\n",
    "#     # Remove emojis\n",
    "#     text = emoji_pattern.sub('', text)\n",
    "#     # Remove any non-ASCII symbols (except regular punctuation)\n",
    "#     text = ''.join(char for char in text if ord(char) < 128 and unicodedata.category(char) != 'Mn')\n",
    "#     return text\n",
    "\n",
    "# def contains_non_ascii(text):\n",
    "#     # Remove emojis and symbols first\n",
    "#     text = remove_emojis_and_symbols(text)\n",
    "#     try:\n",
    "#         # Check if the remaining text contains non-ASCII characters\n",
    "#         text.encode('ascii')\n",
    "#     except UnicodeEncodeError:\n",
    "#         return True\n",
    "#     return False\n",
    "\n",
    "print(\"Total data with non-ASCII chars:\", int(df['text'].apply(contains_non_ascii).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DF size: 1764 --> ['text', 'emotion', 'marah', 'jijik', 'takut', 'senang', 'sedih', 'terkejut', 'biasa']\n",
      "Validation DF size: 365 --> ['text', 'emotion', 'marah', 'jijik', 'takut', 'senang', 'sedih', 'terkejut', 'biasa']\n",
      "Testing DF size: 365 --> ['text', 'emotion', 'marah', 'jijik', 'takut', 'senang', 'sedih', 'terkejut', 'biasa']\n"
     ]
    }
   ],
   "source": [
    "# Stratified split the data\n",
    "def create_stratify_col(df):\n",
    "    # Create 'stratify' column for stratified split\n",
    "    df['stratify'] = df['emotion']\n",
    "\n",
    "    # Identify classes with only one member\n",
    "    single_class = df['emotion'].value_counts()[df['emotion'].value_counts() == 1].index\n",
    "\n",
    "    # Assign a dummy value for the 'stratify' column for these classes\n",
    "    df.loc[df['emotion'].isin(single_class), 'stratify'] = 'dummy'\n",
    "\n",
    "create_stratify_col(df)\n",
    "\n",
    "emotion_lt_7 = df['emotion'].value_counts()[(df['emotion'].value_counts() < 7)].index.tolist()\n",
    "emotion_lt_7_cond = df['emotion'].isin(emotion_lt_7)\n",
    "df_emotion_lt_7 = df[emotion_lt_7_cond]\n",
    "df_emotion_gte_7 = df[~emotion_lt_7_cond]\n",
    "\n",
    "# Split training DF into training and validation DFs\n",
    "if len(split_sizes) == 3:\n",
    "    train_df, val_test_df = train_test_split(df_emotion_gte_7[['text', 'emotion'] + emotion_cols],\n",
    "                                             train_size=split_sizes[0],\n",
    "                                             stratify=df_emotion_gte_7['stratify'],\n",
    "                                             random_state=seed)\n",
    "    \n",
    "    create_stratify_col(val_test_df)\n",
    "    test_size = split_sizes[-1]/(split_sizes[1] + split_sizes[-1])\n",
    "    val_df, test_df = train_test_split(val_test_df[['text', 'emotion'] + emotion_cols],\n",
    "                                        test_size=test_size,\n",
    "                                        stratify=val_test_df['stratify'],\n",
    "                                        random_state=seed)\n",
    "\n",
    "train_df = pd.concat([train_df, df_emotion_lt_7[['text', 'emotion'] + emotion_cols]])\n",
    "\n",
    "print(\"Training DF size:\", len(train_df), \"-->\", train_df.columns.tolist())\n",
    "print(\"Validation DF size:\", len(val_df), \"-->\", val_df.columns.tolist())\n",
    "print(\"Testing DF size:\", len(test_df), \"-->\", test_df.columns.tolist())\n",
    "\n",
    "# dev_df = pd.read_csv(os.path.join(raw_data_dir, f'dev/track_a/{lang}_a.csv'))\n",
    "# dev_df = dev_df.rename(columns=emotion_col_map[lang])\n",
    "# dev_df['emotion'] = None\n",
    "# dev_df = dev_df[['text', 'emotion'] + emotion_cols]\n",
    "# print(\"Dev. DF length:\", len(dev_df), \"-->\", dev_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_non_aug = df[df['aug_go_emotions'] == 0]\n",
    "# df_aug = df[df['aug_go_emotions'] == 1]\n",
    "\n",
    "# print(\"DF non-augmented size:\", len(df_non_aug))\n",
    "# print(\"DF augmented size:\", len(df_aug))\n",
    "# print(\"DF total size:\", len(df_aug) + len(df_non_aug))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Stratified split the data\n",
    "# def create_stratify_col(df):\n",
    "#     # Create 'stratify' column for stratified split\n",
    "#     df.loc[:, 'stratify'] = df['emotion']\n",
    "\n",
    "#     # Identify classes with only one member\n",
    "#     single_class = df['emotion'].value_counts()[df['emotion'].value_counts() <= 2].index\n",
    "\n",
    "#     # Assign a dummy value for the 'stratify' column for these classes\n",
    "#     df.loc[df['emotion'].isin(single_class), 'stratify'] = 'dummy'\n",
    "\n",
    "# def stratify(df):\n",
    "#     create_stratify_col(df)\n",
    "\n",
    "#     emotion_lt_7 = df['emotion'].value_counts()[(df['emotion'].value_counts() < 7)].index.tolist()\n",
    "#     emotion_lt_7_cond = df['emotion'].isin(emotion_lt_7)\n",
    "#     df_emotion_lt_7 = df[emotion_lt_7_cond]\n",
    "#     df = df[~emotion_lt_7_cond]\n",
    "\n",
    "#     # Split training DF into training and validation DFs\n",
    "#     if len(split_sizes) == 3:\n",
    "#         train_df, val_test_df = train_test_split(df,\n",
    "#                                                 train_size=split_sizes[0],\n",
    "#                                                 stratify=df['stratify'],\n",
    "#                                                 random_state=seed)\n",
    "#         create_stratify_col(val_test_df)\n",
    "#         test_size = split_sizes[-1]/(split_sizes[1] + split_sizes[-1])\n",
    "#         val_df, test_df = train_test_split(val_test_df,\n",
    "#                                         test_size=test_size,\n",
    "#                                         stratify=val_test_df['stratify'],\n",
    "#                                         random_state=seed)\n",
    "\n",
    "#     train_df = pd.concat([train_df, df_emotion_lt_7])\n",
    "\n",
    "#     return train_df, val_df, test_df\n",
    "\n",
    "# train_df_non_aug, val_df_non_aug, test_df_non_aug = stratify(df_non_aug)\n",
    "# print(\"Training DF non-augmented size:\", len(train_df_non_aug), \"-->\", train_df_non_aug.columns.tolist())\n",
    "# print(\"Validation DF non-augmented size:\", len(val_df_non_aug), \"-->\", val_df_non_aug.columns.tolist())\n",
    "# print(\"Testing DF non-augmented size:\", len(test_df_non_aug), \"-->\", test_df_non_aug.columns.tolist())\n",
    "# print()\n",
    "\n",
    "# train_df_aug, val_df_aug, test_df_aug = stratify(df_aug)\n",
    "# print(\"Training DF augmented size:\", len(train_df_aug), \"-->\", train_df_aug.columns.tolist())\n",
    "# print(\"Validation DF augmented size:\", len(val_df_aug), \"-->\", val_df_aug.columns.tolist())\n",
    "# print(\"Testing DF augmented size:\", len(test_df_aug), \"-->\", test_df_aug.columns.tolist())\n",
    "# print()\n",
    "\n",
    "# print(f\"Training non-augmented : augmented ratio: {(len(train_df_non_aug)/(len(train_df_non_aug) + len(train_df_aug))*100):.2f}% : {(len(train_df_aug)/(len(train_df_non_aug) + len(train_df_aug))*100):.2f}%\")\n",
    "# print(f\"Validation non-augmented : augmented ratio: {(len(val_df_non_aug)/(len(val_df_non_aug) + len(val_df_aug))*100):.2f}% : {(len(val_df_aug)/(len(val_df_non_aug) + len(val_df_aug))*100):.2f}%\")\n",
    "# print(f\"Testing non-augmented : augmented ratio: {(len(test_df_non_aug)/(len(test_df_non_aug) + len(test_df_aug))*100):.2f}% : {(len(test_df_aug)/(len(test_df_non_aug) + len(test_df_aug))*100):.2f}%\")\n",
    "# print()\n",
    "\n",
    "# train_df = pd.concat([train_df_non_aug, train_df_aug], axis=0)[['text', 'emotion'] + emotion_cols]\n",
    "# val_df = pd.concat([val_df_non_aug, val_df_aug], axis=0)[['text', 'emotion'] + emotion_cols]\n",
    "# test_df = pd.concat([test_df_non_aug, test_df_aug], axis=0)[['text', 'emotion'] + emotion_cols]\n",
    "# print(\"Training DF size:\", len(train_df), \"-->\", train_df.columns.tolist())\n",
    "# print(\"Validation DF size:\", len(val_df), \"-->\", val_df.columns.tolist())\n",
    "# print(\"Testing DF size:\", len(test_df), \"-->\", test_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emotion\n",
       "senang                     65\n",
       "sedih                      40\n",
       "terkejut                   38\n",
       "biasa                      38\n",
       "takut                      37\n",
       "marah                      37\n",
       "jijik                      33\n",
       "senang, terkejut           20\n",
       "marah, jijik               14\n",
       "senang, sedih               7\n",
       "takut, sedih                6\n",
       "marah, sedih                4\n",
       "jijik, takut                3\n",
       "jijik, sedih                3\n",
       "sedih, terkejut             3\n",
       "jijik, terkejut             2\n",
       "takut, terkejut             2\n",
       "senang, sedih, terkejut     2\n",
       "marah, terkejut             2\n",
       "jijik, senang               2\n",
       "marah, takut                2\n",
       "marah, jijik, terkejut      2\n",
       "marah, sedih, terkejut      1\n",
       "takut, senang, terkejut     1\n",
       "takut, senang               1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: ./data/preprocessed_data/track_a/sun_go_emotions_70_15_15_back_translated\n"
     ]
    }
   ],
   "source": [
    "save_dir = os.path.join(preprocessed_data_dir, 'track_a', hf_data_config)\n",
    "\n",
    "!mkdir -p $save_dir\n",
    "\n",
    "train_df.to_csv(os.path.join(save_dir, 'train.csv'), index=False)\n",
    "val_df.to_csv(os.path.join(save_dir, 'val.csv'), index=False)\n",
    "test_df.to_csv(os.path.join(save_dir, 'test.csv'), index=False)\n",
    "# dev_df.to_csv(os.path.join(save_dir, 'dev.csv'), index=False)\n",
    "\n",
    "print(\"Saved to:\", save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Preprocessed Data to Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/alxxtexxr/SemEval2025-Task11-Dataset/commit/4799d120d8565340bc80cd042c36b0501012e520', commit_message='Upload folder using huggingface_hub', commit_description='', oid='4799d120d8565340bc80cd042c36b0501012e520', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/alxxtexxr/SemEval2025-Task11-Dataset', endpoint='https://huggingface.co', repo_type='dataset', repo_id='alxxtexxr/SemEval2025-Task11-Dataset'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_api = HfApi()\n",
    "hf_api.upload_folder(\n",
    "    repo_id=hf_data_id,\n",
    "    repo_type='dataset',\n",
    "    folder_path=save_dir,\n",
    "    path_in_repo=os.path.join('preprocessed_data/track_a', hf_data_config),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alxxtexxr_py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
